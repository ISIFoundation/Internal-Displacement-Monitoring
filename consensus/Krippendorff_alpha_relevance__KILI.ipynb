{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert dataset (Kili)\n",
    "\n",
    "# Define function and metrics for computing Krippendorff's alpha on processed Kili output\n",
    "\n",
    "#### Note that K's alpha measures disagreement, not agreement.\n",
    "#### Final score between 0 and 1 (but can also be negative), where 1 = full reliability (and no disagreement)\n",
    "\n",
    "Alpha is computed on LABELS only (classification part of each task), then on position index OVERLAP (entity extraction part of each task), then on OVERLAP + text SIMILARITY (to cover cases where the labeled texts are similar but not overlapping). Finally the function eval_metric merges everything into one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>EN2652359</td>\n",
       "      <td>EN2422390</td>\n",
       "      <td>EN2718256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labeler</th>\n",
       "      <td>FP</td>\n",
       "      <td>FP</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>NOT_RELEVANT</td>\n",
       "      <td>RELEVANT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1          2\n",
       "id       EN2652359     EN2422390  EN2718256\n",
       "labeler         FP            FP         FP\n",
       "value     RELEVANT  NOT_RELEVANT   RELEVANT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kilidata = pd.read_csv('../data/Kili/relevance_en.csv')\n",
    "kilidata.columns = ['id', 'labeler', 'value']\n",
    "# kilidata = kilidata[kilidata.value!='N_A']\n",
    "kilidata.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kilidata.id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT_RELEVANT    136\n",
       "RELEVANT        122\n",
       "N_A               6\n",
       "Name: value, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kilidata.value.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Krippendorf's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics for agreement\n",
    "def nominal_metric(a, b):\n",
    "    return a != b\n",
    "\n",
    "\n",
    "def interval_metric(a, b):\n",
    "    return (a-b)**2\n",
    "\n",
    "\n",
    "def ratio_metric(a, b):\n",
    "    return ((a-b)/(a+b))**2\n",
    "\n",
    "\n",
    "def krippendorff_alpha(units, metric=interval_metric):\n",
    "\n",
    "    n = sum(len(pv) for pv in units.values())  # number of pairable values\n",
    "    print (f'n: {n}')\n",
    "    \n",
    "    if n == 0:\n",
    "        raise ValueError(\"No items to compare.\")\n",
    "    \n",
    "    Do = 0.\n",
    "    for grades in units.values():\n",
    "        Du = sum(metric(gi, gj) for gi in grades for gj in grades)\n",
    "        Do += Du/float(len(grades)-1)\n",
    "    Do /= float(n)\n",
    "    print (f'Do: {Do}')\n",
    "\n",
    "    if Do == 0:\n",
    "        return 1.\n",
    "\n",
    "    De = 0.\n",
    "    for g1 in units.values():\n",
    "        for g2 in units.values():\n",
    "            De += sum(metric(gi, gj) for gi in g1 for gj in g2)\n",
    "    De /= float(n*(n-1))\n",
    "    print (f'De: {De}')\n",
    "\n",
    "    return 1.-Do/De if (Do and De) else 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each task, prepare data for computing alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: RELEVANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "EN2401380    [NOT_RELEVANT, NOT_RELEVANT, NOT_RELEVANT]\n",
       "EN2433725        [RELEVANT, NOT_RELEVANT, NOT_RELEVANT]\n",
       "EN2434557            [NOT_RELEVANT, RELEVANT, RELEVANT]\n",
       "EN2445660                          [RELEVANT, RELEVANT]\n",
       "EN2468074    [NOT_RELEVANT, NOT_RELEVANT, NOT_RELEVANT]\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create assetID-labels table for a given task \n",
    "df_type = kilidata[['id', 'labeler', 'value']]\n",
    "df_type = df_type.groupby(['id', 'labeler']).last().reset_index()\n",
    "\n",
    "units = df_type.groupby('id').apply(lambda x: x.value.values)\n",
    "units_type = units[units.map(lambda x: len(x)>1)]\n",
    "units_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 103\n",
      "Do: 0.13592233009708737\n",
      "De: 0.5010470207500476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7287234042553192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def type_metric_str(a, b):\n",
    "    (a,b) = sorted((a,b))\n",
    "    if a==b:\n",
    "        return 0        # 0:agreement \n",
    "    elif a=='NOT_RELEVANT' and b=='N_A': # nr and na are same\n",
    "        return 0\n",
    "    else:\n",
    "        return 1        # 1:disagreement\n",
    "\n",
    "krippendorff_alpha(units_type.to_dict(), type_metric_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
