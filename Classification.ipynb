{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate classification models\n",
    "\n",
    "This notebook is used to load data for the classification task and evaluate different classification models.\n",
    "\n",
    "Before loading the content, you must run the notebook Data-Preparation.ipynb to scrape articles not shared with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv('data/assets_scraped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data for the Type classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df_nocontent = pd.read_csv('data/type_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df = pd.merge(type_df_nocontent, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def text_basic_clean(text):\n",
    "    text = text.replace('\\n\\nâ€¢', '').replace('\\n\\n', '')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = text.replace('  ', ' ')\n",
    "    text = ' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                     for word in text.split()\n",
    "                     if word not in stopwords.words('english') and word.isalpha()])\n",
    "    return text\n",
    "\n",
    "type_df['content_clean'] = type_df['content'].apply(lambda x: text_basic_clean(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester():\n",
    "    \n",
    "    def __init__(self, model, param_grid={}):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                            analyzer='word', ngram_range=(1, 4), \n",
    "                            min_df = 5, stop_words='english',norm='l2')\n",
    "    \n",
    "    def run_val_test(self, seed):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=seed, stratify = y)\n",
    "\n",
    "        tfidf_matrix = self.tfidf.fit_transform(X_train).toarray()\n",
    "        x_test = self.tfidf.transform(X_test).toarray()\n",
    "        cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=seed)\n",
    "        gs = GridSearchCV(self.model, param_grid=self.param_grid,\n",
    "                          cv=cv, scoring='roc_auc')\n",
    "        gs.fit(tfidf_matrix, y_train)\n",
    "        predicted_prob = gs.predict_proba(x_test)\n",
    "        \n",
    "        return gs.best_score_, metrics.roc_auc_score(y_test, predicted_prob[:,1])\n",
    "    \n",
    "    def run_many(self, n_runs=50):\n",
    "        val_scores = []\n",
    "        test_scores = []\n",
    "        for seed in tqdm(range(n_runs), file=sys.stdout):\n",
    "            val_score, test_score = self.run_val_test(seed)\n",
    "            val_scores.append(val_score)\n",
    "            test_scores.append(test_score)\n",
    "        return val_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model():\n",
    "    xgb_clf = xgb.XGBClassifier(use_label_encoder=False)\n",
    "    args = {'learning_rate': 0.3, 'colsample_bytree': 0.8, 'scale_pos_weight': 3, \n",
    "            'n_jobs': -1, 'n_estimators': 300, 'max_depth': 8, 'subsample': 0.8, 'verbosity': 0}\n",
    "    return xgb_clf.set_params(**args)\n",
    "\n",
    "def get_rf():\n",
    "    rf = RandomForestClassifier(min_samples_leaf = 5, \n",
    "                                n_estimators = 200,\n",
    "                                class_weight = 'balanced_subsample',\n",
    "                                criterion = 'gini',\n",
    "                                random_state = 42)\n",
    "    return rf\n",
    "\n",
    "lr_model = LogisticRegression(penalty='l2', C=0.1, class_weight='balanced', solver='liblinear')\n",
    "svm_model = SVC(kernel='rbf', probability=True, class_weight='balanced')\n",
    "\n",
    "models = [\n",
    "    ('Multinomial Naive Bayes', MultinomialNB(), {'alpha': [0.5, 1, 1.5]}),\n",
    "    ('XGBoost', xgb_model(), {'max_depth': [3,4,5], 'n_estimators': [10, 20]}),\n",
    "    ('Random Forest', get_rf(), {'min_samples_leaf': [3,4,5,6]}),\n",
    "    ('Logistic Regression', lr_model, {'C': [0.1, 1, 2]}),\n",
    "    ('Support Vector Machine', svm_model, {'C': [0.1, 1, 2]})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: [Both & News] v.s. Summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(type_df.content_clean)\n",
    "y = np.array(type_df.value.map(lambda x: 0 if x=='SUMMARY' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_scenario1 = {}\n",
    "for name, model, param_grid in models:\n",
    "    print('Testing model', name)\n",
    "    tester = Tester(model, param_grid)\n",
    "    results_scenario1[name] = tester.run_many()\n",
    "    print([np.mean(r) for r in results_scenario1[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({k: [np.mean(v[0]), np.mean(v[1])]\n",
    "              for (k,v) in results_scenario1.items()}, index=['Validation', 'Test']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: [Both & Summary] v.s. News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(type_df.content_clean)\n",
    "y = np.array(type_df.value.map(lambda x: 0 if x=='NEWS' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_scenario2 = {}\n",
    "for name, model, param_grid in models:\n",
    "    print('Testing model', name)\n",
    "    tester = Tester(model, param_grid)\n",
    "    results_scenario2[name] = tester.run_many()\n",
    "    print([np.mean(r) for r in results_scenario2[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({k: [np.mean(v[0]), np.mean(v[1])]\n",
    "              for (k,v) in results_scenario2.items()}, index=['Validation', 'Test']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senario 3: News v.s. Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df2 = type_df.copy()\n",
    "type_df2 = type_df2[type_df2.value != 'BOTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(type_df2.content_clean)\n",
    "y = np.array(type_df2.value.map(lambda x: 0 if x=='SUMMARY' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_scenario3 = {}\n",
    "for name, model, param_grid in models:\n",
    "    print('Testing model', name)\n",
    "    tester = Tester(model)\n",
    "    results_scenario3[name] = tester.run_many()\n",
    "    print([np.mean(r) for r in results_scenario3[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({k: [np.mean(v[0]), np.mean(v[1])]\n",
    "              for (k,v) in results_scenario3.items()}, index=['validation', 'test']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df_nocontent = pd.read_csv('data/relevance_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df = pd.merge(relevance_df_nocontent, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(relevance_df.relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def text_basic_clean(text):\n",
    "    text = text.replace('\\n\\nâ€¢', '').replace('\\n\\n', '')\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = text.replace('  ', ' ')\n",
    "    text = ' '.join([lemmatizer.lemmatize(word.lower()) \n",
    "                     for word in text.split() \n",
    "                     if word not in stopwords.words('english') and word.isalpha()])\n",
    "    return text\n",
    "\n",
    "relevance_df['content_clean'] = relevance_df['content'].apply(lambda x: text_basic_clean(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(relevance_df.content_clean)\n",
    "y = np.array(relevance_df.relevance.map(lambda x: 0 if x=='NOT_RELEVANT' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_relevance = {}\n",
    "for name, model, param_grid in models:\n",
    "    print('Testing model', name)\n",
    "    tester = Tester(model)\n",
    "    results_relevance[name] = tester.run_many()\n",
    "    print([np.mean(r) for r in results_relevance[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({k: [np.mean(v[0]), np.mean(v[1])]\n",
    "              for (k,v) in results_relevance.items()}, index=['validation', 'test']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
